import torch
from transformers import LlamaForCausalLM, AutoTokenizer
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight
import logging
import argparse
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.cuda.amp import autocast, GradScaler
import random
import torch.nn.functional as F

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model_and_tokenizer(model_name: str, device: str):
    """Load the LLaMA model and tokenizer, ensuring the tokenizer has a padding token."""
    logger.info(f"Loading model: {model_name}")
    model = LlamaForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Set padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info("Setting pad_token to eos_token.")
    
    model.config.pad_token_id = tokenizer.pad_token_id
    
    model = model.to(device)
    return model, tokenizer

def prepare_balanced_data(dataset, num_samples=50000):
    """Prepare a balanced dataset."""
    human_texts = [(text, 1) for text, source in zip(dataset['text'], dataset['source']) if source == 'human']
    ai_texts = [(text, 0) for text, source in zip(dataset['text'], dataset['source']) if source == 'ai']
    
    # Ensure we have enough samples of each class
    num_samples_per_class = min(len(human_texts), len(ai_texts), num_samples // 2)
    
    # Randomly sample from each class
    human_samples = random.sample(human_texts, num_samples_per_class)
    ai_samples = random.sample(ai_texts, num_samples_per_class)
    
    # Combine and shuffle
    combined_samples = human_samples + ai_samples
    random.shuffle(combined_samples)
    
    texts, labels = zip(*combined_samples)
    return list(texts), list(labels)

def prepare_data(texts, labels, tokenizer, max_length=512):
    """Prepare data for causal language modeling with improved prompts and label encoding."""
    prompts = [f"Is the following text human-written or AI-generated? Text: {text} Answer: {'human-written' if label == 1 else 'AI-generated'}" for text, label in zip(texts, labels)]
    
    # Ensure tokenizer has padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    encoded_inputs = tokenizer(prompts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
    
    # Create label tensor
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)
    label_tensor = torch.tensor(encoded_labels)
    
    return encoded_inputs['input_ids'], encoded_inputs['attention_mask'], label_tensor


import numpy as np
from torch.cuda.amp import GradScaler, autocast

from sklearn.preprocessing import LabelEncoder
def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device):
    """Train the model using batched data and class weights."""
    model.train()
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    scaler = GradScaler() if device == 'cuda' else None
    
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        total_loss = 0
        model.train()
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            
            optimizer.zero_grad()
            
            with autocast(enabled=device=="cuda"):
                # Forward pass
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                
                # Shift logits and labels for next token prediction
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = input_ids[..., 1:].contiguous()
                
                # Flatten the tokens
                loss_fct = CrossEntropyLoss(ignore_index=model.config.pad_token_id)
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            if scaler:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validate
        val_loss = validate_model(model, val_loader, device)
        val_losses.append(val_loss)
        
        logger.info(f"Epoch {epoch+1} completed. "
                    f"Train Loss: {avg_train_loss:.4f}, "
                    f"Val Loss: {val_loss:.4f}")

    plot_losses(train_losses, val_losses)
    return model

def validate_model(model, val_loader, device):
    """Validate the model."""
    model.eval()
    total_loss = 0
    loss_fct = CrossEntropyLoss(ignore_index=model.config.pad_token_id)
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids, attention_mask, _ = [b.to(device) for b in batch]

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # Shift logits and labels for next token prediction
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = input_ids[..., 1:].contiguous()
            
            # Flatten the tokens
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            total_loss += loss.item()

    avg_loss = total_loss / len(val_loader)
    return avg_loss

# Make sure to import CrossEntropyLoss
from torch.nn import CrossEntropyLoss
def plot_losses(train_losses, val_losses):
    """Plot training and validation losses."""
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss Over Time')
    plt.legend()
    plt.savefig('loss_plot.png')
    logger.info("Loss plot saved as 'loss_plot.png'")

def evaluate_model(model, data_loader, device, tokenizer):
    """Evaluate model and compute metrics with improved reporting and error handling."""
    model.eval()
    all_preds = []
    all_labels = []
    prediction_counts = {"human": 0, "AI": 0}
    true_label_counts = {"human": 0, "AI": 0}

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids, attention_mask, labels = [b.to(device) for b in batch]

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits[:, -1, :]  # Get the last token's logits
            predicted_token_id = torch.argmax(logits, dim=-1)
            predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_id)
            
            # Convert predictions to binary (1 for 'human', 0 for 'AI')
            predictions = []
            for token in predicted_tokens:
                if 'human' in token.lower():
                    predictions.append(1)
                    prediction_counts["human"] += 1
                else:
                    predictions.append(0)
                    prediction_counts["AI"] += 1
            
            # Extract true labels from the input (assuming the label is the last token)
            true_labels = tokenizer.batch_decode(labels[:, -1].unsqueeze(-1), skip_special_tokens=True)
            true_labels = [1 if 'human' in label.lower() else 0 for label in true_labels]
            
            for label in true_labels:
                if label == 1:
                    true_label_counts["human"] += 1
                else:
                    true_label_counts["AI"] += 1

            all_preds.extend(predictions)
            all_labels.extend(true_labels)

    logger.info(f"Prediction counts: {prediction_counts}")
    logger.info(f"True label counts: {true_label_counts}")
    
    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    logger.info("\nConfusion Matrix:")
    logger.info(cm)

    # Compute metrics
    accuracy = accuracy_score(all_labels, all_preds)
    
    # Handle cases where precision, recall, and f1 are undefined
    if len(set(all_preds)) == 1:  # If all predictions are the same
        logger.warning("Model predicted only one class. Precision, Recall, and F1 score may be undefined.")
        if all_preds[0] == 1:  # If all predictions are 'human'
            precision = 1.0 if all_labels[0] == 1 else 0.0
            recall = 1.0 if all(label == 1 for label in all_labels) else sum(all_labels) / len(all_labels)
        else:  # If all predictions are 'AI'
            precision = 1.0 if all_labels[0] == 0 else 0.0
            recall = 1.0 if all(label == 0 for label in all_labels) else (len(all_labels) - sum(all_labels)) / len(all_labels)
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    else:
        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)

    logger.info("\nClassification Report:")
    logger.info(f"Accuracy: {accuracy:.4f}")
    logger.info(f"Precision: {precision:.4f}")
    logger.info(f"Recall: {recall:.4f}")
    logger.info(f"F1 Score: {f1:.4f}")

    return accuracy, precision, recall, f1


def main(args):
    # Set random seeds for reproducibility
    torch.manual_seed(args.seed)
    random.seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
    logger.info(f"Using device: {device}")

    # Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer(args.model_name, device)

    # Load dataset
    logger.info("Loading dataset")
    dataset = load_dataset("artem9k/ai-text-detection-pile", split="train")
    logger.info(f"Dataset size: {len(dataset)}")

    # Prepare balanced dataset
    texts, labels = prepare_balanced_data(dataset, num_samples=args.num_samples)
    logger.info(f"Prepared balanced dataset with {len(texts)} samples")

    # Split data
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.2, random_state=args.seed)
    val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=args.seed)

    # Prepare data
    logger.info("Preparing data")
    train_input_ids, train_attention_mask, train_labels = prepare_data(train_texts, train_labels, tokenizer)
    val_input_ids, val_attention_mask, val_labels = prepare_data(val_texts, val_labels, tokenizer)
    test_input_ids, test_attention_mask, test_labels = prepare_data(test_texts, test_labels, tokenizer)

    # Create datasets
    train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)
    val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)
    test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)

    logger.info(f"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples, testing on {len(test_dataset)} samples")

    # Train model
    logger.info("Starting training")
    model = train_model(model, train_loader, val_loader, args.num_epochs, args.learning_rate, device)

    # Evaluate on training data
    logger.info("Evaluating on training data")
    train_accuracy, train_precision, train_recall, train_f1 = evaluate_model(model, train_loader, device, tokenizer)

    # Evaluate on test data
    logger.info("Evaluating on test data")
    test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device, tokenizer)

    # Print results
    logger.info("\nResults:")
    logger.info(f"{'Metric':<15}{'Train':<15}{'Test':<15}")
    logger.info(f"{'-'*45}")
    logger.info(f"{'Accuracy':<15}{train_accuracy:<15.4f}{test_accuracy:<15.4f}")
    logger.info(f"{'Precision':<15}{train_precision:<15.4f}{test_precision:<15.4f}")
    logger.info(f"{'Recall':<15}{train_recall:<15.4f}{test_recall:<15.4f}")
    logger.info(f"{'F1 Score':<15}{train_f1:<15.4f}{test_f1:<15.4f}")

    # Save model
    if args.save_model:
        logger.info(f"Saving model to {args.save_path}")
        model.save_pretrained(args.save_path)
        tokenizer.save_pretrained(args.save_path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fine-tune LLaMA for text classification")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B", help="Name of the pre-trained model")
    parser.add_argument("--num_samples", type=int, default=20000, help="Number of samples to use from the dataset")
    parser.add_argument("--batch_size", type=int, default=8, help="Batch size for training")
    parser.add_argument("--num_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--learning_rate", type=float, default=5e-6, help="Learning rate")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--cpu", action="store_true", help="Use CPU instead of GPU")
    parser.add_argument("--save_model", action="store_true", help="Save the fine-tuned model")
    parser.add_argument("--save_path", type=str, default="./fine_tuned_model", help="Path to save the fine-tuned model")
    
    args = parser.parse_args()
    main(args)
