""" This script, referred to as Baseline 1, loads the meta-llama/Llama-3.2-1B model and a state-of-the-art dataset for AI text detection. It evaluates whether a given text is AI-generated or human-written, returning "real" or "fake" based on model predictions. The script processes the full text for evaluation, logs each input text along with the model's decision, and computes accuracy. The evaluation is customizable by specifying the number of samples and other parameters via command-line arguments."""
import torch
from transformers import LlamaForCausalLM, AutoTokenizer
from datasets import load_dataset
from sklearn.metrics import accuracy_score
import logging
import argparse
from sklearn.model_selection import train_test_split

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set up file handler to log text and model decisions to a file
file_handler = logging.FileHandler('model_decisions.log')
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

def load_model_and_tokenizer(model_name: str):
    """Load the LLaMA model and tokenizer."""
    try:
        logger.info(f"Loading {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = LlamaForCausalLM.from_pretrained(
            model_name, 
            device_map="auto", 
            load_in_8bit=True, 
            torch_dtype=torch.float16
        )

        # Ensure the tokenizer has a padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as pad token
            logger.info("Setting pad_token to eos_token.")
        
        model.config.pad_token_id = tokenizer.pad_token_id
        
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error loading model {model_name}: {str(e)}")
        raise
def classify_text(model: torch.nn.Module, tokenizer: AutoTokenizer, text: str) -> str:
    """Classify text as 'real' or 'fake'."""
    try:
        # Use the full text instead of truncating it to 100 characters
        prompt = f"Is the following text human-written or AI-generated? Text: '{text}'. Reply with 'real' or 'fake'."
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)  # Increased max_length
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=2,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()
        
        if 'real' in generated_text:
            return 'real'
        elif 'fake' in generated_text:
            return 'fake'
        else:
            logger.warning(f"Generated text did not contain a valid 'real' or 'fake' classification: {generated_text}")
            return None
    except Exception as e:
        logger.error(f"Error in classify_text: {str(e)}")
        return None

def evaluate_model(model: torch.nn.Module, tokenizer: AutoTokenizer, dataset, num_samples: int):
    """Evaluate model on the dataset, compute accuracy, and log text and model decisions."""
    correct_predictions = 0
    total_samples = 0

    for idx, (text, source) in enumerate(dataset):  # Unpack tuple (text, source)
        if idx >= num_samples:  # Ensure you only process up to num_samples
            break
        true_label = "real" if source == "human" else "fake"
        predicted_label = classify_text(model, tokenizer, text)
        
        # Log the text, true label, and model's decision
        logger.info(f"Text: {text[:100]}... | True Label: {true_label} | Model Decision: {predicted_label}")
        
        if predicted_label == true_label:
            correct_predictions += 1
        
        total_samples += 1
        if total_samples % 100 == 0:
            logger.info(f"Processed {total_samples} samples")

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0
    logger.info(f"Accuracy: {accuracy:.4f}")
    return accuracy

def main(args):
    # Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer(args.model_name)

    # Load AI text detection dataset (using "train" split)
    dataset = load_dataset("artem9k/ai-text-detection-pile", split="train")  # Use the 'train' split
    logger.info(f"Loaded dataset with {len(dataset)} samples.")

    # Convert dataset to a list for compatibility with train_test_split
    dataset_list = list(zip(dataset['text'], dataset['source']))  # List of tuples (text, source)

    # Split the "train" data into train/test sets
    train_data, test_data = train_test_split(dataset_list, test_size=0.2, random_state=args.seed)
    logger.info(f"Using {len(test_data)} samples for evaluation.")

    # Evaluate model on the test set
    accuracy = evaluate_model(model, tokenizer, test_data, args.num_samples)
    logger.info(f"Final Accuracy: {accuracy:.4f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Simple LLaMA-based Text Detection")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B", help="Name of the model to use")
    parser.add_argument("--num_samples", type=int, default=100, help="Number of samples to evaluate")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    args = parser.parse_args()

    main(args)
