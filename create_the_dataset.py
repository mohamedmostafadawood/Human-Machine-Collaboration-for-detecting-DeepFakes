# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qo6pnZ9DT8NHUulsmeShLm2mv5Lka_Vv
"""

import pandas as pd
import random
# Assuming you uploaded a file named 'dataset.csv'
df = pd.read_csv('Training_Essay_Data.csv')

# Show the first few rows of the dataset
#df.head()

# Reasons text is considered deepfake
deepfake_text_reasons = [
    "Inconsistent Tone and Style",
    "Lack of Contextual Understanding",
    "Repetitive Patterns",
    "Factual Errors",
    "Unnatural Language Constructs",
    "Overreliance on Templates or Patterns",
    "Difficulty with Complex Topics",
    "Inconsistent Character Development",
    "Lack of Emotional Nuance",
    "Inability to Adapt to Different Writing Styles"
]

# Reasons text is considered human-generated
human_text_reasons = [
    "Natural Fluency",
    "Contextual Understanding",
    "Diverse Vocabulary and Style",
    "Personal Nuances",
    "Ability to Correct Errors",
    "Ability to Generate Original Ideas",
    "Adaptation to Different Contexts",
    "Ability to Learn and Grow",
    "Cultural Nuances",
    "Subjective Perspectives"
]

# Function to generate random reasons
def get_reason(generated):
    if generated == 1:  # Deepfake text
        return f"I think it is a deepfake text because {random.choice(deepfake_text_reasons)}"
    elif generated == 0:  # Human-generated text
        return f"I think it is human generated text because {random.choice(human_text_reasons)}"

# Apply the function to each row in the 'generated' column
df['generated'] = df['generated'].apply(get_reason)

# Add the phrase to the start of each 'text' entry
df['text'] = df['text'].apply(lambda x: "Is this text a deepfake text or human generated text? " + x)

#df['generated'] = df['generated'].replace({
 #   1: "I think it is a deepfake text because bla bla",
 #   0: "I think it is human generated text because bla bla"
#})

#df = df.rename(columns={'generated': 'reason'})
new_df = df.copy()
new_df.tail()

import json
import pandas as pd

DEFAULT_SYSTEM_PROMPT = 'You are an Interactive smart AI assistant. The user will provide you with a text and you should guess if it is a deepfake text or human generated text and most importantly you should provide reasons in each case to support your claim'

def create_dataset(text, generated):
    return {
        "messages": [
            {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
            {"role": "user", "content": text},
            {"role": "assistant", "content": generated},
        ]
    }

if __name__ == "__main__":
    # Write the JSONL file
    with open("train.jsonl", "w") as f:
        for _, row in new_df.iterrows():
            example_str = json.dumps(create_dataset(row['text'], row['generated']))
            f.write(example_str + "\n")

# List to store all JSON objects
data_list = []

# Path to your jsonl file
file_path = "train.jsonl"

# Open the jsonl file and load it into a list
with open(file_path, 'r') as f:
    for line in f:
        data = json.loads(line)
        data_list.append(data)

# Convert the list of JSON objects to a DataFrame
df = pd.DataFrame(data_list)

# Display the first few rows of the DataFrame
print(df.tail())

!pip install tiktoken

import json
import tiktoken # for token counting
import numpy as np
from collections import defaultdict

data_path = "train.jsonl"

# Load the dataset
with open(data_path, 'r', encoding='utf-8') as f:
    dataset = [json.loads(line) for line in f]

# Initial dataset stats
print("Num examples:", len(dataset))
print("First example:")
for message in dataset[0]["messages"]:
    print(message)

# Format error checks
format_errors = defaultdict(int)

for ex in dataset:
    if not isinstance(ex, dict):
        format_errors["data_type"] += 1
        continue

    messages = ex.get("messages", None)
    if not messages:
        format_errors["missing_messages_list"] += 1
        continue

    for message in messages:
        if "role" not in message or "content" not in message:
            format_errors["message_missing_key"] += 1

        if any(k not in ("role", "content", "name", "function_call", "weight") for k in message):
            format_errors["message_unrecognized_key"] += 1

        if message.get("role", None) not in ("system", "user", "assistant", "function"):
            format_errors["unrecognized_role"] += 1

        content = message.get("content", None)
        function_call = message.get("function_call", None)

        if (not content and not function_call) or not isinstance(content, str):
            format_errors["missing_content"] += 1

    if not any(message.get("role", None) == "assistant" for message in messages):
        format_errors["example_missing_assistant_message"] += 1

if format_errors:
    print("Found errors:")
    for k, v in format_errors.items():
        print(f"{k}: {v}")
else:
    print("No errors found")

encoding = tiktoken.get_encoding("cl100k_base")

# not exact!
# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3
    return num_tokens

def num_assistant_tokens_from_messages(messages):
    num_tokens = 0
    for message in messages:
        if message["role"] == "assistant":
            num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

def print_distribution(values, name):
    print(f"\n#### Distribution of {name}:")
    print(f"min / max: {min(values)}, {max(values)}")
    print(f"mean / median: {np.mean(values)}, {np.median(values)}")
    print(f"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}")

# Warnings and tokens counts
n_missing_system = 0
n_missing_user = 0
n_messages = []
convo_lens = []
assistant_message_lens = []

for ex in dataset:
    messages = ex["messages"]
    if not any(message["role"] == "system" for message in messages):
        n_missing_system += 1
    if not any(message["role"] == "user" for message in messages):
        n_missing_user += 1
    n_messages.append(len(messages))
    convo_lens.append(num_tokens_from_messages(messages))
    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))

print("Num examples missing system message:", n_missing_system)
print("Num examples missing user message:", n_missing_user)
print_distribution(n_messages, "num_messages_per_example")
print_distribution(convo_lens, "num_total_tokens_per_example")
print_distribution(assistant_message_lens, "num_assistant_tokens_per_example")
n_too_long = sum(l > 16385 for l in convo_lens)
print(f"\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning")

# Pricing and default n_epochs estimate
MAX_TOKENS_PER_EXAMPLE = 16385

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

n_epochs = TARGET_EPOCHS
n_train_examples = len(dataset)
if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:
    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)
elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:
    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)

n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)
print(f"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training")
print(f"By default, you'll train for {n_epochs} epochs on this dataset")
print(f"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens")