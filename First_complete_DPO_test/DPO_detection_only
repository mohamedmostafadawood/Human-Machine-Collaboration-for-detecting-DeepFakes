import os
import torch
from transformers import LlamaForCausalLM, AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm
import wandb
import json
import argparse
import logging
from torch.optim import AdamW
import random
import numpy as np
from sklearn.metrics import accuracy_score
from huggingface_hub import HfFolder
from accelerate import Accelerator

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize the accelerator
accelerator = Accelerator(mixed_precision="fp16")

def check_tensor(tensor, tensor_name):
    """Helper function to check for NaN or Inf in a tensor"""
    if torch.isnan(tensor).any() or torch.isinf(tensor).any():
        logger.error(f"{tensor_name} contains NaN or Inf values.")

def get_device():
    return "cuda" if torch.cuda.is_available() else "cpu"

def load_model_and_tokenizer(model_name, model_type="llama"):
    logger.info(f"Loading {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if model_type == "llama":
        model = LlamaForCausalLM.from_pretrained(
            model_name, 
            device_map="auto", 
            load_in_8bit=True, 
            torch_dtype=torch.float16
        )
    else:
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=2,  # Binary classification for 'real' and 'fake'
            device_map="auto", 
            torch_dtype=torch.float16
        )

    # Ensure the tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as the pad token if no pad token exists
        logger.info("Setting pad_token to eos_token.")
    
    model.config.pad_token_id = tokenizer.pad_token_id  # Ensure model knows about the pad token
    
    model = accelerator.prepare(model)
    
    return model, tokenizer


def classify_text(model, tokenizer, text):
    prompt = f"Is the following text human-written or AI-generated? Text: '{text[:100]}'. Reply with 'real' or 'fake'."
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
    
    # Move inputs to the correct device
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=2,  
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()
    
    # Extract decision from generated text
    if 'real' in generated_text:
        return 'real'
    elif 'fake' in generated_text:
        return 'fake'
    else:
        logger.warning("Generated text did not contain a valid 'real' or 'fake' classification.")
        return None

def judge_preference(judge_model, judge_tokenizer, text, decision1, decision2):
    """Judge model compares the two decisions (real/fake) and outputs a preference"""
    input_text = f"Text: {text[:100]}\nDecision 1: {decision1}\nDecision 2: {decision2}\nWhich decision is better? Choose 1 for first or 2 for second."
    inputs = judge_tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {k: v.to(judge_model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = judge_model(**inputs)
        logits = outputs.logits
        choice = torch.argmax(logits).item() + 1  # 1 for first, 2 for second
        return choice

def dpo_training_step(agent, optimizer, tokenizer, text, decision, other_decision, judge_preference, accumulation_steps=4):
    agent.train()
    optimizer.zero_grad()

    inputs = tokenizer(decision, return_tensors="pt", truncation=True, max_length=128)
    inputs = {k: v.to(agent.device) for k, v in inputs.items()}

    outputs = agent(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss / accumulation_steps

    if judge_preference == 1:  # If this decision was preferred
        margin = 0.1
        other_inputs = tokenizer(other_decision, return_tensors="pt", truncation=True, max_length=128)
        other_inputs = {k: v.to(agent.device) for k, v in other_inputs.items()}
        other_outputs = agent(**other_inputs, labels=other_inputs["input_ids"])
        loss = torch.max(torch.tensor(0.0).to(agent.device), other_outputs.loss - loss + margin)
    elif judge_preference == 2:  # If the other decision was preferred
        margin = 0.1
        other_inputs = tokenizer(other_decision, return_tensors="pt", truncation=True, max_length=128)
        other_inputs = {k: v.to(agent.device) for k, v in other_inputs.items()}
        other_outputs = agent(**other_inputs, labels=other_inputs["input_ids"])
        loss = torch.max(torch.tensor(0.0).to(agent.device), loss - other_outputs.loss + margin)

    accelerator.backward(loss)
    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)

    return loss.item()

def train_agents_with_dpo(agent1, agent2, judge_model, tokenizer1, tokenizer2, judge_tokenizer, texts, args, accumulation_steps=4):
    optimizer1 = AdamW(agent1.parameters(), lr=args.learning_rate)
    optimizer2 = AdamW(agent2.parameters(), lr=args.learning_rate)
    
    dataloader = DataLoader(texts, batch_size=args.batch_size, shuffle=True)

    wins1, wins2, ties = 0, 0, 0
    correct_detections_agent1, correct_detections_agent2 = 0, 0
    total_samples = 0

    for epoch in range(args.num_epochs):
        total_loss1, total_loss2 = 0, 0

        for batch in tqdm(dataloader, desc=f"Training epoch {epoch+1}"):
            for text in batch:
                decision1 = classify_text(agent1, tokenizer1, text)
                decision2 = classify_text(agent2, tokenizer2, text)

                if decision1 is None or decision2 is None:
                    continue

                total_samples += 1
                true_label = "real" if random.random() > 0.5 else "fake"  # Simulating true label for now
                if decision1 == true_label:
                    correct_detections_agent1 += 1
                if decision2 == true_label:
                    correct_detections_agent2 += 1

                judge_choice = judge_preference(judge_model, judge_tokenizer, text, decision1, decision2)

                if judge_choice == 1:
                    loss1 = dpo_training_step(agent1, optimizer1, tokenizer1, text, decision1, decision2, 1, accumulation_steps)
                    loss2 = dpo_training_step(agent2, optimizer2, tokenizer2, text, decision2, decision1, 2, accumulation_steps)
                    wins1 += 1
                elif judge_choice == 2:
                    loss1 = dpo_training_step(agent1, optimizer1, tokenizer1, text, decision1, decision2, 2, accumulation_steps)
                    loss2 = dpo_training_step(agent2, optimizer2, tokenizer2, text, decision2, decision1, 1, accumulation_steps)
                    wins2 += 1
                else:
                    loss1 = dpo_training_step(agent1, optimizer1, tokenizer1, text, decision1, decision2, 0, accumulation_steps)
                    loss2 = dpo_training_step(agent2, optimizer2, tokenizer2, text, decision2, decision1, 0, accumulation_steps)
                    ties += 1

                total_loss1 += loss1
                total_loss2 += loss2

        avg_loss1 = total_loss1 / len(dataloader)
        avg_loss2 = total_loss2 / len(dataloader)
        detection_rate_agent1 = correct_detections_agent1 / total_samples
        detection_rate_agent2 = correct_detections_agent2 / total_samples

        logger.info(f"Epoch {epoch+1}: Loss Agent1 {avg_loss1:.4f}, Loss Agent2 {avg_loss2:.4f}")
        logger.info(f"Detection Rate: Agent1 {detection_rate_agent1:.4f}, Agent2 {detection_rate_agent2:.4f}")
        logger.info(f"Wins: Agent1 {wins1}, Agent2 {wins2}, Ties {ties}")

        wandb.log({
            "epoch": epoch + 1,
            "loss_agent1": avg_loss1,
            "loss_agent2": avg_loss2,
            "detection_rate_agent1": detection_rate_agent1,
            "detection_rate_agent2": detection_rate_agent2,
            "wins_agent1": wins1,
            "wins_agent2": wins2,
            "ties": ties
        })

    return agent1, agent2

def load_and_prepare_data(args):
    dataset = load_dataset("tweet_eval", "sentiment", split="train", trust_remote_code=True)
    texts = dataset["text"]
    texts = [text for text in texts if len(text.strip()) > 10]
    return texts[:args.num_samples]

def main(args):
    # Initialize wandb
    wandb.init(project="llama-dpo-human-vs-ai-detection", config=args)

    agent1, tokenizer1 = load_model_and_tokenizer(args.model_name_1, model_type="llama")
    agent2, tokenizer2 = load_model_and_tokenizer(args.model_name_2, model_type="llama")
    judge_model, judge_tokenizer = load_model_and_tokenizer(args.judge_model_name, model_type="classification")

    texts = load_and_prepare_data(args)

    improved_agent1, improved_agent2 = train_agents_with_dpo(
        agent1, agent2, judge_model, tokenizer1, tokenizer2, judge_tokenizer, texts, args
    )

    improved_agent1.save_pretrained(f"{args.output_dir}/agent1")
    tokenizer1.save_pretrained(f"{args.output_dir}/agent1")
    improved_agent2.save_pretrained(f"{args.output_dir}/agent2")
    tokenizer2.save_pretrained(f"{args.output_dir}/agent2")
    logger.info(f"Models saved to {args.output_dir}")

    wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DPO Training for Human vs AI Text Detection")
    parser.add_argument("--model_name_1", type=str, default="meta-llama/Llama-3.2-1B", help="First model")
    parser.add_argument("--model_name_2", type=str, default="meta-llama/Llama-3.2-1B", help="Second model")
    parser.add_argument("--judge_model_name", type=str, default="meta-llama/Llama-3.2-1B", help="Judge model")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-6, help="Learning rate")
    parser.add_argument("--num_samples", type=int, default=50, help="Number of samples")
    parser.add_argument("--output_dir", type=str, default="./llama_dpo_models", help="Directory to save the models")

    args = parser.parse_args()
    main(args)
