
import torch
from transformers import LlamaForCausalLM, AutoTokenizer
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import logging
import argparse
from sklearn.model_selection import train_test_split
from torch.nn import functional as F
from torch.optim import AdamW
from tqdm import tqdm
import matplotlib.pyplot as plt

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_model_and_tokenizer(model_name: str):
    """Load the LLaMA model and tokenizer."""
    try:
        logger.info(f"Loading {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = LlamaForCausalLM.from_pretrained(
            model_name, 
            device_map="auto", 
            load_in_8bit=False,
            torch_dtype=torch.float32
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("Setting pad_token to eos_token.")
        
        model.config.pad_token_id = tokenizer.pad_token_id
        
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error loading model {model_name}: {str(e)}")
        raise

def prepare_input(text: str, tokenizer: AutoTokenizer):
    """Prepare input for the model."""
    prompt = f"Is the following text human-written or AI-generated? Text: '{text}'. Reply with 'real' or 'fake'."
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512, padding=True)
    return inputs

def sanity_check_loss(loss_value):
    """Perform a sanity check on the loss value."""
    if not (0 <= loss_value <= 10):  # Adjust this range based on your specific use case
        logger.warning(f"Unusual loss value detected: {loss_value}")
    return 0 <= loss_value < float('inf')

def train_model(model: torch.nn.Module, tokenizer: AutoTokenizer, train_data, val_data, num_epochs: int, learning_rate: float):
    """Train the model using Cross-Entropy Loss with additional checks."""
    model.train()
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        total_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        for text, label in tqdm(train_data, desc=f"Epoch {epoch+1}/{num_epochs}"):
            inputs = prepare_input(text, tokenizer)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # Prepare target
            target = torch.tensor([1 if label == "human" else 0], dtype=torch.long).to(model.device)

            # Forward pass
            outputs = model(**inputs)
            logits = outputs.logits[:, -1, :]  # Get logits for the last token
            
            loss = model.loss

            # Compute Cross-Entropy Loss
            loss = F.cross_entropy(logits, target)

            # Sanity check
            if not sanity_check_loss(loss.item()):
                logger.error(f"Invalid loss detected: {loss.item()}. Skipping this batch.")
                continue

            # Backward pass (compute gradients)
            optimizer.zero_grad()


            # Update model parameters
            optimizer.step()

            total_loss += loss.item()
            logger.debug(f"Batch loss: {loss.item()}")

            # Calculate accuracy
            predictions = torch.argmax(logits, dim=-1)
            correct_predictions += (predictions == target).sum().item()
            total_samples += target.size(0)

        avg_train_loss = total_loss / len(train_data)
        train_losses.append(avg_train_loss)
        
        train_accuracy = correct_predictions / total_samples
        train_accuracies.append(train_accuracy)
        
        # Validate on validation set
        val_loss, val_accuracy = validate_model(model, tokenizer, val_data)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)
        
        logger.info(f"Epoch {epoch+1} completed. "
                    f"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
                    f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)
    return model

def validate_model(model: torch.nn.Module, tokenizer: AutoTokenizer, val_data):
    """Validate the model on a validation set."""
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_samples = 0
    
    with torch.no_grad():
        for text, label in tqdm(val_data, desc="Validating"):
            inputs = prepare_input(text, tokenizer)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            target = torch.tensor([1 if label == "human" else 0], dtype=torch.long).to(model.device)

            outputs = model(**inputs)
            logits = outputs.logits[:, -1, :]

            loss = F.cross_entropy(logits, target)
            total_loss += loss.item()

            predictions = torch.argmax(logits, dim=-1)
            correct_predictions += (predictions == target).sum().item()
            total_samples += target.size(0)

    avg_loss = total_loss / len(val_data)
    accuracy = correct_predictions / total_samples
    return avg_loss, accuracy

def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):
    """Plot training and validation losses and accuracies."""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
    
    ax1.plot(train_losses, label='Training Loss')
    ax1.plot(val_losses, label='Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss Over Time')
    ax1.legend()
    
    ax2.plot(train_accuracies, label='Training Accuracy')
    ax2.plot(val_accuracies, label='Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Training and Validation Accuracy Over Time')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_metrics.png')
    logger.info("Training metrics plot saved as 'training_metrics.png'")

def evaluate_model(model: torch.nn.Module, tokenizer: AutoTokenizer, test_data):
    """Evaluate model on the test set and compute various metrics."""
    model.eval()
    all_preds = []
    all_labels = []

    for text, label in tqdm(test_data, desc="Evaluating"):
        inputs = prepare_input(text, tokenizer)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits[:, -1, :]
            prediction = torch.argmax(logits, dim=-1).item()
        
        all_preds.append(prediction)
        all_labels.append(1 if label == "human" else 0)

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='binary')
    recall = recall_score(all_labels, all_preds, average='binary')
    f1 = f1_score(all_labels, all_preds, average='binary')

    logger.info(f"Test Accuracy: {accuracy:.4f}")
    logger.info(f"Test Precision: {precision:.4f}")
    logger.info(f"Test Recall: {recall:.4f}")
    logger.info(f"Test F1 Score: {f1:.4f}")

    return accuracy, precision, recall, f1

def main(args):
    # Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer(args.model_name)

    # Load AI text detection dataset
    dataset = load_dataset("artem9k/ai-text-detection-pile", split="train")
    logger.info(f"Loaded dataset with {len(dataset)} samples.")

    # Convert dataset to a list and take only 20,000 samples
    dataset_list = list(zip(dataset['text'], dataset['source']))[:20000]

    # Split the data into train/val/test sets
    train_data, temp_data = train_test_split(dataset_list, test_size=0.4, random_state=args.seed)
    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=args.seed)
    logger.info(f"Training on {len(train_data)} samples, validating on {len(val_data)} samples, testing on {len(test_data)} samples.")

    # Train the model
    model = train_model(model, tokenizer, train_data, val_data, args.num_epochs, args.learning_rate)

    # Evaluate model on the test set
    accuracy, precision, recall, f1 = evaluate_model(model, tokenizer, test_data)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CEL-Optimized AI Text Detection")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B", help="Name of the model to use")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--num_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="Learning rate for optimization")
    args = parser.parse_args()

    main(args)
